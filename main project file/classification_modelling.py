import glob
import json
import logging
import math
import os
import re
import time
from pathlib import Path
from statistics import mode

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import xgboost as xgb
from scipy.sparse import data
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,
                             confusion_matrix, f1_score, mean_absolute_error,
                             mean_squared_error, precision_score, r2_score,
                             recall_score)
from sklearn.model_selection import GridSearchCV, KFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from tabulate import tabulate

import modelling as md
import tabular_data as td

le = LabelEncoder()

def get_baseline_classification_score(model, data_subsets):
    """Tunes the hyperparameters of a classification model and saves the information.
    Args:
        model (class): The classification model to be implemented as a baseline.
        data_subsets (list): List in the form [X_train, y_train, X_val, y_val, X_test, y_test].
        folder (str): The directory path of where to save the data.
    Returns:
        model: model data generated by training data 
        params (dict): hyperparameters used for the model model
        metrics (dict): Training, validation and testing performance metrics.
    """
    logging.info('Calculating baseline score...')
    #fit the data baes on labels and features from the train set
    #print(data_subsets)
    #print(data_subsets[1])
    model = model(max_iter=1000).fit(data_subsets[0], data_subsets[1])
    #predict the labels based on the training subset features
    y_train_pred = model.predict(data_subsets[0])
    cfm_train = create_confusion_matrix(data_subsets[1], y_train_pred)
    print(cfm_train)
    #predict the labels based on the val subset featrues
    y_val_pred = model.predict(data_subsets[2])
    cfm_val = create_confusion_matrix(data_subsets[3], y_val_pred)
    print(cfm_val)
    #predict the labels based on the test subset features
    y_test_pred = model.predict(data_subsets[4])
    cfm_test = create_confusion_matrix(data_subsets[5], y_test_pred)
    print(cfm_test)
    #get the model parameters
    params = model.get_params()
    # calculate the performance metrics by comparing the prediction to actual scores
    
    metrics = get_all_data_metrics(data_subsets, y_train_pred, y_val_pred, y_test_pred)

    

    return model, params, metrics 

def get_all_data_metrics(data_subsets, y_train_pred, y_validation_pred, y_test_pred):
    """Generates model data metrics for classification data
    Args:
        data_subsets: data used to train the model in list form [X_train, Y_train, X_val, y_val, X_test, y_test]
        y_train_pred: The predicted data based on training dataset
        y_validation_pred: The predicted data based on validation dataset
        y_test_pred: The predicted data based on test dataset
    Returns: dict of all metrics"""
    train_acc_score, train_prec_score, train_rec_score, train_f1_score = get_performance_metrics(data_subsets[1], y_train_pred)
    val_acc_score, val_prec_score, val_rec_score, val_f1_score = get_performance_metrics(data_subsets[3], y_validation_pred)
    test_acc_score, test_prec_score, test_rec_score, test_f1_score = get_performance_metrics(data_subsets[5], y_test_pred)

    metrics = { "train_acc_score" : train_acc_score,
                "train_prec_score" : train_prec_score, 
                "train_rec_score" : train_rec_score, 
                "train_f1_scorer" : train_f1_score,
                "val_acc_score" : val_acc_score,
                "val_prec_score" : val_prec_score, 
                "val_rec_score" : val_rec_score, 
                "val_f1_score"  : val_f1_score,
                "test_acc_score" : test_acc_score,
                "test_prec_score" : test_prec_score, 
                "test_rec_score"  : test_rec_score, 
                "test_f1_score"  : test_f1_score}  
    
    return metrics

def get_all_data_metrics_transformed(data_subsets, y_train_pred, y_validation_pred, y_test_pred):
    """ Generates model data metrics for classification data, normalising the string data to numerical data first
    Args:
        data_subsets: data used to train the model in list form [X_train, Y_train, X_val, y_val, X_test, y_test]
        y_train_pred: The predicted data based on training dataset
        y_validation_pred: The predicted data based on validation dataset
        y_test_pred: The predicted data based on test dataset
    Returns: dict of all metrics"""

    normalised_train = le.fit_transform(data_subsets[1])
    normalised_val = le.fit_transform(data_subsets[3])
    normalised_test = le.fit_transform(data_subsets[5])
    
    train_acc_score, train_prec_score, train_rec_score, train_f1_score = get_performance_metrics(normalised_train, y_train_pred)
    val_acc_score, val_prec_score, val_rec_score, val_f1_score = get_performance_metrics(normalised_val, y_validation_pred)
    test_acc_score, test_prec_score, test_rec_score, test_f1_score = get_performance_metrics(normalised_test, y_test_pred)
    
    metrics = { "train_acc_score" : train_acc_score,
                "train_prec_score" : train_prec_score, 
                "train_rec_score" : train_rec_score, 
                "train_f1_scorer" : train_f1_score,
                "val_acc_score" : val_acc_score,
                "val_prec_score" : val_prec_score, 
                "val_rec_score" : val_rec_score, 
                "val_f1_score"  : val_f1_score,
                "test_acc_score" : test_acc_score,
                "test_prec_score" : test_prec_score, 
                "test_rec_score"  : test_rec_score, 
                "test_f1_score"  : test_f1_score}  
    
    
    return metrics

def create_confusion_matrix(true_labels, pred_labels):
    """Creates a confusion matrix based on true and predicted labels
    Args:
        true_labels: true labels
        pred_labels: predicted labels from model
    Returns:
        Normalised confusion matrix"""
    cf_matrix = confusion_matrix(true_labels, pred_labels)
    return cf_matrix / cf_matrix.sum()

def visualise_and_save_confusion_matrix(confusion_matrix, path=None): 
    """Displays a pre-generated confusion matrix and optionally saves the confusion matrix if path to containing folder is given
    Args:
        confusion_matrix: a confusion matrix generated elsewhere
        path (optional): if given then the confusion matrix would be saved to the folder """
    display = ConfusionMatrixDisplay(confusion_matrix)
    display.plot()
    plt.show()
    if path != None:
        plt.savefig(f'{path}\cfm.png')


def get_confusion_matrix(true_labels, pred_labels, path=None):
    """Create confusion matrix and crates a plot, can optionally save the plot
    Args: 
        true_labels: true labels
        pred_labels: predicted labels from model
        confusion_matrix: a confusion matrix generated elsewhere
        path (optional): if given then the confusion matrix would be saved to the folder """
    cf = create_confusion_matrix(true_labels, pred_labels)
    return visualise_and_save_confusion_matrix(cf, path)

def get_performance_metrics(true_labels, pred_labels):
    """Generates performance metrics for classification estimators
    Args: 
        true_labels: true labels 
        pred_labels: predicted labels from model
    Returns:
        tuple: accuracy score, precision score, recall score, f1 score"""
    acc_score = accuracy_score(true_labels, pred_labels)
    prec_score = precision_score(true_labels, pred_labels, average='micro')
    rec_score = recall_score(true_labels, pred_labels, average='micro')
    f1_scr = f1_score(true_labels, pred_labels, average='micro')
    return acc_score, prec_score, rec_score, f1_scr

def tune_classification_model_hyperparameters(model_name, data_subsets, hyperparameters):
    """"Tunes the hyperparameters of a classification model and saves the hyperparameters.
    Args:
        model (class): The classification model to be as a baseline.
        data_sets (list): List in the form [X_train, y_train, X_validation, y_validation, X_test, y_test].
        
    Returns:
        metrics (dict): Training, validation and testing performance metrics.
    """
    logging.info('Performing GridSearch with KFold...')
    model = model_name(random_state=1)
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    grid_search = GridSearchCV(model, hyperparameters, cv=kfold)
    # 
    model = grid_search.fit(data_subsets[0], data_subsets[1])
    
    #xgboost needs conversion of test to numerical data, so need to apply this
    
    y_train_pred = model.predict(data_subsets[0])
    y_validation_pred = model.predict(data_subsets[2])
    y_test_pred = model.predict(data_subsets[4])

    params = model.best_params_
  
    metrics = get_all_data_metrics(data_subsets, y_train_pred, y_validation_pred, y_test_pred)

    return model, params, metrics

def tune_classification_model_hyperparameters_with_normalisation(model_name, data_subsets, hyperparameters):
    """Tunes the hyperparameters of a classification model and saves the hyperparameters, while also normalising the data to compare data generated from a model which requires normalised categorical data as numerical data.
    Args:
        model (class): The classification model to be as a baseline.
        data_sets (list): List in the form [X_train, y_train, X_validation, y_validation, X_test, y_test].
        
    Returns:
        metrics (dict): Training, validation and testing performance metrics.
    """
    logging.info('Performing GridSearch with KFold...')
    model = model_name(random_state=1)
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    grid_search = GridSearchCV(model, hyperparameters, cv=kfold)

    #xgboost needs conversion of test to numerical data, so need to apply this
    
    normalised_train = le.fit_transform(data_subsets[1])
    print(f'normalised_train = {normalised_train}, len_train_x ={len(data_subsets[0])} len_train_y = {len(data_subsets[1])}, len_norm_train ={len(normalised_train)}')
    normalised_val = le.fit_transform(data_subsets[3])
    normalised_test = le.fit_transform(data_subsets[5])

    model = grid_search.fit(data_subsets[0], normalised_train)
    
    y_train_pred = model.predict(data_subsets[0])
    y_validation_pred = model.predict(data_subsets[2])
    y_test_pred = model.predict(data_subsets[4])

    params = model.best_params_
  
    metrics = get_all_data_metrics_transformed(data_subsets, y_train_pred, y_validation_pred, y_test_pred)

    return model, params, metrics

def evaluate_all_models(data_subsets):
    """Tunes the hyperparameters of DecisionTreeRegressor, RandomForestClassifier
        and XGBClassifier.
        Args: data_subsets: data subsets as list in the form [X_train, y_train, X_validation, y_validation, X_test, y_test].
        Returns: list of dictionaries containing model, parameters and metrics for all of the mo
    """
    
    decision_tree = tune_classification_model_hyperparameters(
        DecisionTreeClassifier,
        data_subsets,
        dict(
            criterion=['gini'], 
        max_depth=[6], 
        min_samples_split= [8],
        ccp_alpha= [0.01]
        )
    ) 
    
    
   # save_model(dt_model, dt_params, dt_metrics, Path(f'{task_folder}/DecisionTreeClassifier/mean_scores')) 

    random_forest = tune_classification_model_hyperparameters(
        RandomForestClassifier,
        data_subsets,
        dict(
            criterion=['entropy'],
            min_samples_split= [3],
            n_estimators=[100],
            max_depth=[9],
            bootstrap=[True],
            max_samples = [30],
            )
        )  
   # save_model(rf_model, rf_params, rf_metrics, Path(f'{task_folder}/RandomForestClassifier/mean_scores'))

    xg_boost =  tune_classification_model_hyperparameters_with_normalisation(
        xgb.XGBClassifier,
        data_subsets,
        dict(
            gamma= [1], 
            learning_rate= [0.5], 
            max_depth= [5], 
            min_child_weight= [6], 
            n_estimators= [33], 
            reg_alpha= [6], 
            reg_lambda=[0.1]
            )
    )
    #save_model(xg_model, xg_params, xg_metrics, Path(f'{task_folder}/XGBClassifier/mean_scores'))
    
    combined_dictionaries_list = decision_tree + random_forest + xg_boost
    return combined_dictionaries_list

def exclude_outliers(split_data, contamination:float):
    """Function to remove outliers from training dataset
    Args: 
        split_data: list of data to exclude outliers, the first 2 list elements must be X_train, y_train
        contamination: the amount of contamination of the data set, i.e. the proportion of outliers in the data set.
    Returns: 
        tuple of X_train and y_train with outliers excluded
        """
    x_train = split_data[0]
    len_x_train = len(x_train)
    print(f'len x_train = {len_x_train}')
    y_train = split_data[1]
    len_y_train = len(y_train)
    print(f'len y_train = {len_y_train}')
    iso = IsolationForest(contamination=contamination)
    yhat = iso.fit_predict(x_train)
    # select all rows that are not outliers
    mask = yhat != -1
    x_train, y_train = x_train.iloc[mask, :], y_train[mask]
    
    return x_train, y_train

# def alpha_and_gamma_test_for_xgboost(model_name, hyperparameters, data_subsets, kfold):
#     model = model_name(random_state=1)
#     kfold = KFold(n_splits=5, shuffle=True, random_state=42)
#     grid_search = GridSearchCV(model, hyperparameters, cv=kfold)
#     model = grid_search.fit(data_subsets[0], data_subsets[1])
    
#     y_train_pred = model.predict(data_subsets[0])
#     y_validation_pred = model.predict(data_subsets[2])
#     y_test_pred = model.predict(data_subsets[4])
#     {"n_estimators": list(33, 34, 35, 36), "max_depth": list(1, 2, 3), "min_child_weight": list(4, 5, 6), "reg_alpha": list(0, 0.001, 0.005, 0.01, 0.05), "gamma_reg": list(0, 0.25, 0.5, 1.0), "learning_rate": list(0.1, 0.25, 0.5)}
#     return grid_search.cv_results_

def load_airbnb(dataframe_input, label):
    """Generates AirBnB data for training classification model
    Args:
        dataframe_input: the dataframe containing the airbnb data
        label: the label correspodning to y dataset for model training
    Returns:
        dataframe excluding the label
        datafram column contining label data"""
    df = dataframe_input.drop(label, axis = 1)
    df = df.drop(columns=['ID', 'Title','Description','Amenities','Location','url'])
    return df, dataframe_input[label]

def split_the_data(features, labels, test_to_rest_ratio, validation_to_test_ratio, random_state):
    '''Splits data into train:validation:test subsets.
    Args:   features: features subset of data as list
            labels: labels subset of data as list
            test_to_rest_ratio: the proportion of the data to be taken as the test data set
            validation_to_test_ratio: what proportion to split the test subset into validation and test
            random_state: the random state for the data splitting
    Returns: split data as X_train, y_train, X_validation, y_validation, X_test, y_test
    
    '''
    #split the data into test and train data; the 0.3 describes the data which is apportioned to the test set 
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_to_rest_ratio, random_state=random_state)
    #print(y_train)
    #resplit the test data again to get a final 15 % for both test and validation
    X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=validation_to_test_ratio, random_state=random_state)
    
    return [X_train, y_train, X_validation, y_validation, X_test, y_test]

def save_model(model, hyperparameters, metrics, folder_for_files):
    """saves the information of a tuned regression model.
    Args:
        model (class): Saved as a .joblib file.
        hyperparameters (dict): Saved as a .json file.
        metrics (dict): Saved as a .json file.
        folder (str): The directory path of where to save the data.
    """
    joblib.dump(model, f'{folder_for_files}/model.joblib')
    with open(f'{folder_for_files}/hyperparameters.json', 'a') as outfile:
        json.dump(hyperparameters, outfile)
    with open(f'{folder_for_files}/metrics.json', 'a') as outfile:
        json.dump(metrics, outfile)    

def evaluate_models_multiple_times(num_iter, seed):
    """ Iterates through the modelling process a predermined number of times based on a pseudorandom seed
    Args:
        num_iter: number of iterations 
        seed: the pseudorandom order of random states to iterate through in sequence"""
    BL_metrics_list_of_dicts = []
    DT_params_list_of_dicts = []
    DT_metrics_list_of_dicts = []
    RF_params_list_of_dicts = []
    RF_metrics_list_of_dicts = []
    XG_params_list_of_dicts = []
    XG_metrics_list_of_dicts = []

    np.random.seed(seed)
    random_states=  np.random.randint(99, size=num_iter)
    
    df = pd.read_csv(Path('AirbnbDataSci/tabular_data/AirBnbData.csv'))
    df_2 = td.clean_tabular_data(df)
    print(df_2)
    features, labels = td.load_airbnb(df_2, 'Category') 
    #increase training size due to overfitting of more complex models
    for iteration in random_states: 
        split_data = split_the_data(features, labels, 0.25, 0.5, random_state=iteration)
        
        
        BL_model, BL_params, BL_metrics = get_baseline_classification_score(LogisticRegression, split_data)
        BL_metrics_list_of_dicts.append(BL_metrics)
        models_list = evaluate_all_models(split_data)
        #print(XG_params_list_of_dicts)
        DT_model = models_list[0]
        DT_params_list_of_dicts.append(models_list[1])
        DT_metrics_list_of_dicts.append(models_list[2])
        RF_model = models_list[3]
        RF_params_list_of_dicts.append(models_list[4])
        RF_metrics_list_of_dicts.append(models_list[5])
        XG_model = models_list[6]
        XG_params_list_of_dicts.append(models_list[7])
        XG_metrics_list_of_dicts.append(models_list[8])

    
    # print(f'BL_metrics_list_of_dicts is {BL_metrics_list_of_dicts}')
    # print(DT_params_list_of_dicts)
    BL_metrics = get_aggregate_scores(BL_metrics_list_of_dicts)

    save_model(BL_model, BL_params, BL_metrics, Path(r"C:\Users\marko\DS Projects\AirBnB-Data-Modelling\main project file\models\classification\logistic_regression\mean_scores"))
    
    DT_metrics = get_aggregate_scores(DT_metrics_list_of_dicts)
    
    # #get the most common parameters of all of the iterations
    DT_common_params = mode([ sub['max_depth'] for sub in DT_params_list_of_dicts ])
    DT_common_params = {'max_depth' : DT_common_params}
    # print(DT_common_params)
    save_model(DT_model, DT_common_params, DT_metrics, Path(r"C:\Users\marko\DS Projects\AirBnB-Data-Modelling\main project file\models\classification\DecisionTreeClassifier\mean_scores"))

    RF_metrics = get_aggregate_scores(RF_metrics_list_of_dicts)
    
    
    # #get the most common parameters of all of the iterations
    RF_common_n_estimators = mode([ sub['n_estimators'] for sub in RF_params_list_of_dicts ])
    RF_common_max_depth = mode([ sub['max_depth'] for sub in RF_params_list_of_dicts ])
    RF_common_bootstrap =mode([ sub['bootstrap'] for sub in RF_params_list_of_dicts ])
    RF_common_max_samples =mode([ sub['max_samples'] for sub in RF_params_list_of_dicts ])
    RF_common_params = {'n_estimators': RF_common_n_estimators,
                        'max_depth' : RF_common_max_depth,
                        'bootstrap' : RF_common_bootstrap,
                        'max_samples': RF_common_max_samples}
    

    # print(RF_common_params)
    save_model(RF_model, RF_common_params, RF_metrics, Path(r"C:\Users\marko\DS Projects\AirBnB-Data-Modelling\main project file\models\classification\RandomForestClassifier\mean_scores"))

    XG_metrics = get_aggregate_scores(XG_metrics_list_of_dicts)

    
    
    #get the most common parameters of all of the iterations
    XG_common_n_estimators = mode([ sub['n_estimators'] for sub in XG_params_list_of_dicts ])
    XG_common_max_depth = mode([ sub['max_depth'] for sub in XG_params_list_of_dicts ])
    XG_common_min_child_weight = mode([ sub['min_child_weight'] for sub in XG_params_list_of_dicts ])
    XG_common_gamma =mode([ sub['gamma'] for sub in XG_params_list_of_dicts ])
    XG_common_learning_rate =mode([ sub['learning_rate'] for sub in XG_params_list_of_dicts ])
    XG_common_alpha_penalty =mode([ sub['reg_alpha'] for sub in XG_params_list_of_dicts ])
    XG_common_params = {'n_estimators' : XG_common_n_estimators,
                        'max_depth' : XG_common_max_depth,
                        'min_child_weight' : XG_common_min_child_weight,
                        'gamma': XG_common_gamma,
                        'learning_rate': XG_common_learning_rate,
                        'reg_alpha': XG_common_alpha_penalty}
    

    print(XG_common_params)
    
    save_model(XG_model, XG_common_params, XG_metrics, Path(r'C:\Users\marko\DS Projects\AirBnB-Data-Modelling\main project file\models\classification\XGBClassifier\mean_scores_2'))

def get_aggregate_scores(list_of_dictionaries):
    """Get average scores from multiple iterations of the modelling process
    Args: 
        list_of_dictionaries: list of dictionaries generated during evaluate_models_multiple_times execution
    Returns:
        dictionary of aggregate scores
        """
    list_of_train_acc = [b["train_acc_score"] for b in list_of_dictionaries]

    train_mean_acc = np.mean(list_of_train_acc)
    train_std_acc = np.std(list_of_train_acc)
    train_var_acc = (train_std_acc / train_mean_acc) * 100

    list_of_validation_acc = [a['val_acc_score'] for a in list_of_dictionaries]
    val_mean_acc = np.mean(list_of_validation_acc)
    val_std_acc = np.std(list_of_validation_acc)
    val_var_acc = (val_std_acc / val_mean_acc) * 100

    list_of_test_acc = [a['test_acc_score'] for a in list_of_dictionaries]
    test_mean_acc = np.mean(list_of_test_acc)
    test_std_acc = np.std(list_of_test_acc)
    test_var_acc = (test_std_acc / test_mean_acc) * 100
    
    validation_generalisation_score = val_mean_acc / train_mean_acc 
    test_generalisation_score = test_mean_acc / train_mean_acc
    # print(BL_mean_acc)
    # print(BL_std_acc)
    # print(BL_var_acc)
    
    metrics = { 'train_mean_acc': train_mean_acc,
                'train_std_acc': train_std_acc,
                'train_var_acc': train_var_acc,
               
                'val_mean_acc': val_mean_acc,
                'val_std_acc': val_std_acc,
                'val_var_acc': val_var_acc,
                
                'test_mean_acc' : test_mean_acc,
                'test_std_acc' : test_std_acc,
                'test_var_acc' : test_var_acc,

                'validation_generalisation_score': validation_generalisation_score,
                'test_generalisation_score' : test_generalisation_score
                }
    return metrics

def find_best_model():
    """Searches through the regression_models directory to find the model
        with the smallest RMSE value for the validation set (best model).
        Args: None
    Returns:
        best_model (class): Loads the model.joblib file.
        best_hyperparameters (dict): Loads the hyperparameters.json file.
        best_metrics (dict): Loads the metrics.json file.
    """
    logging.info('Finding best model...')

    paths = glob.glob(r'models/classification/*/mean_scores/metrics.json')
    accuracy_dict = {}
    for path in paths:
        print(f'path is {path}')
        model = path[22:-25]
        print(f'model is {model}')
        with open(path) as file:
            metrics = json.load(file)
        accuracy_dict[model] = metrics["val_mean_acc"] 

    model_name = max(accuracy_dict, key=accuracy_dict.get)
    
    model = joblib.load(f'models/classification/{model_name}/mean_scores/model.joblib')
    with open(f'models/classification/{model_name}/mean_scores/hyperparameters.json', 'rb') as file:
            hyperparameters = json.load(file)
    with open(f'models/classification/{model_name}/mean_scores/metrics.json', 'rb') as file:
            metrics = json.load(file)
    best_model = f'The best model is {model_name}!'
    return model, hyperparameters, metrics, best_model

if __name__ == "__main__":
    #evaluate_models_multiple_times(20, 1)
    df = pd.read_csv(Path('AirbnbDataSci/tabular_data/AirBnbData.csv'))
    df_2 = td.clean_tabular_data(df)
    print(df_2)
    features, labels = td.load_airbnb(df_2, 'Category') 
    split_data = split_the_data(features, labels, 0.25, 0.5, random_state=42)
    get_baseline_classification_score(LogisticRegression, split_data)
    
    # best_model = find_best_model()
    # print(best_model)


